---
title: "Stat 585 Lab 4: Team 1"
author: "Qing He, Earl Hur, Kellie McClernon, Xin Zhang"
date: "April 17, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

See our group github [repository](https://github.com/Alleansa/STAT585_LAB4) for all files.

## 1. Reading the data

We could collect approximately 1000 observations from the Iowa data portal for liquors sale without an API key.

The code to obtin the liquirs sale is described as follow:
```{r}
library(tidyverse)
# Get the api address
url <- "https://data.iowa.gov/resource/m3tr-qhgy.json?county=Story"

# Make 1000 list of stores in Story county
story_list <- jsonlite::read_json(url)

# Get the variable names using list.flatten
new.distinct.names <- story_list %>% 
  purrr::map(.x, 
             .f=~names(rbind.data.frame(rlist::list.flatten(.x),0)))

#  Get the observations from variables
unlisted <- story_list %>% 
  purrr::map(.f = ~rbind.data.frame(unlist(.x, recursive=T, use.names=T)))

# Make a list of data frames
unlisted.info <- purrr::map2(unlisted,
                             new.distinct.names,
                             .f= ~purrr::set_names(.x, .y))

# Combine data frames in the list
story_dat <- do.call(plyr::rbind.fill, unlisted.info)

# We do not need these any more.
rm(url, story_list, new.distinct.names, unlisted, unlisted.info)
```

We can also obtain data directly from *data.iowa.gov* website.

```{r liquor_samp, warning=FALSE}
url_csv <- "https://data.iowa.gov/resource/m3tr-qhgy.csv"
liquor1000 <- readr::read_csv(url_csv)
```

If we want more observations then we can sign up for an API key and login to access all [Iowa data](https://data.iowa.gov/login) repositories. Then if our key was "our_key", our call would be as follows.

```{r with_key, eval = FALSE}
#key <- "app_token=our_key"
#url_key <- sprintf("%s&$$%s", url_csv, key)
#all_liquor <- readr::read_csv(url_key) # THis code does not work unless we have our own key...
```

Or we could use an actual API key. However, we are still only getting 1000 observations. For some reason our output appears to be throttled.

```{r real_key, message=FALSE, warning=FALSE}
our_key <- "app_token=X1QYLARnM0NJ7ykwNJ2aH1ig5"
query <- "city=Ames"
grp1_key <- sprintf("%s?$$%s&%s", url_csv, our_key,query)
all_liquor <- readr::read_csv(grp1_key) #Looks the same as the one we obtained in the first step. This code looks a lot simpler than the first one! 
```

## 2. Cleaning the data

### 2.1 data cleaning for spatial varibles

```{r}
## DATA PREPROCESSING
load(file = "./data/rawstory.RData")

#obtain the lon,lat information from store.location column. 
story %>% 
   mutate(Lat = as.numeric(qdapRegex::rm_between(Store.Location, '(', ',', extract=TRUE)),
          Lon = as.numeric(qdapRegex::rm_between(Store.Location, ',', ')', extract=TRUE)))->story_data

#remove stores with no lon,lat information, take the average of obtained lon,lat for each store since some store at the same location reported different lon,lat value.  
 story_data %>% 
   group_by(Store.Number,Store.Name) %>%
   summarise(Lon = mean(Lon,na.rm = 1),
             Lat = mean(Lat,na.rm = 1)) -> store_info
 
 store_info <- store_info[!duplicated(store_info$Store.Number),]
 
 #replace old the lon.lat info
 story_data %>% 
   select(-Lon,-Lat,-Store.Name) %>% 
   left_join(store_info,by='Store.Number') -> story_data

```

### 2.2 Data cleaning for temporal data

```{r}
# Convert Date into date class, add variables month, year and mon_yr for future plots
story_data %>% mutate(Date = lubridate::mdy(Date),
                      City = tolower(City)) %>% 
  filter(Lon !='NaN')-> story_data

story_clean <- story_data %>% mutate(Month = lubridate::month(Date, label = TRUE),
                                     Year = lubridate::year(Date),
                                     Mon_Yr = lubridate::floor_date(Date, unit = "month"))
```


## 3. Shiny app

```{r run_shiny}
#library(shiny)
#runApp("shiny")
```

